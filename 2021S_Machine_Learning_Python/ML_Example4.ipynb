{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Midterm Assignment \n",
    "by Ming Nie mn2984\n",
    "\n",
    "This assignment is based on my own knowledge and my understanding of the class notes, skicit learn official website and wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In answering each of the following questions please include a) the question as a markdown header in your Jupyter notebook, b)  the raw code that you used to generate any results, tables, or figures, and c) the top ten or fewer rows of the data-frame (do not include more than ten rows for any table in your report).\n",
    "\n",
    "Include any plots or figures generated from your code as well.\n",
    "\n",
    "Use the spam dataset and variable descriptions in the files/Mid-Term Exam folder of the course website to answer the following questions.   \n",
    "\n",
    "1.Import the spam dataset and print the first six rows.  \n",
    "\n",
    "2.Read through the documentation of the original dataset here:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "\n",
    "The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?\n",
    "\n",
    "ANSWER: The three variables I choose are word_freq_free:(commercials always use this to attract customers), char_freq_!:(spam uses this to draw your attention, in formal emails, people tend to avoid the use of \"!\"),\tchar_freq_$:(marketing ads tend to use this to inform the receiver the price of their goods/service). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\14249\\OneDrive\\Columbia\\$ML\\midterm\\spam_dataset.csv\")\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make:\n",
      "word_freq_address:\n",
      "word_freq_all:\n",
      "word_freq_3d:\n",
      "word_freq_our:\n",
      "word_freq_over:\n",
      "word_freq_remove:\n",
      "word_freq_internet:\n",
      "word_freq_order:\n",
      "word_freq_mail:\n",
      "word_freq_receive:\n",
      "word_freq_will:\n",
      "word_freq_people:\n",
      "word_freq_report:\n",
      "word_freq_addresses:\n",
      "word_freq_free:\n",
      "word_freq_business:\n",
      "word_freq_email:\n",
      "word_freq_you:\n",
      "word_freq_credit:\n",
      "word_freq_your:\n",
      "word_freq_font:\n",
      "word_freq_000:\n",
      "word_freq_money:\n",
      "word_freq_hp:\n",
      "word_freq_hpl:\n",
      "word_freq_george:\n",
      "word_freq_650:\n",
      "word_freq_lab:\n",
      "word_freq_labs:\n",
      "word_freq_telnet:\n",
      "word_freq_857:\n",
      "word_freq_data:\n",
      "word_freq_415:\n",
      "word_freq_85:\n",
      "word_freq_technology:\n",
      "word_freq_1999:\n",
      "word_freq_parts:\n",
      "word_freq_pm:\n",
      "word_freq_direct:\n",
      "word_freq_cs:\n",
      "word_freq_meeting:\n",
      "word_freq_original:\n",
      "word_freq_project:\n",
      "word_freq_re:\n",
      "word_freq_edu:\n",
      "word_freq_table:\n",
      "word_freq_conference:\n",
      "char_freq_;:\n",
      "char_freq_(:\n",
      "char_freq_[:\n",
      "char_freq_!:\n",
      "char_freq_$:\n",
      "char_freq_#:\n",
      "capital_run_length_average:\n",
      "capital_run_length_longest:\n",
      "capital_run_length_total:\n",
      "spam\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "count = 0\n",
    "for i in df.columns:\n",
    "    count += 1\n",
    "    print(i)\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Visualize the univariate distribution of each of the variables in the previous question.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df ['word_freq_free:'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df ['char_freq_!:'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df ['char_freq_$:'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df ['spam'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".\n",
    "\n",
    "ANSWER: K Nearest Neighbor Classification; \n",
    "logistic regression; \n",
    "penalized logistic regression;\n",
    "support vector machine;\n",
    "decision tree classifier;\n",
    "random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Describe the importance of training and test data.  Why do we separate data into these subsets?\n",
    "\n",
    "ANSWER: We use training test split to prevent overfitting. If we keep training our model on all available data, we probably would get a \"perfect\" model that can work very well on our data. This is partly because we are tries to explain the noise of the data, which is the part cannot be explained. If we do this, we are adding unneccesary parameters and complexity into our model. As a result, the prediction of the model is compromised by overfitting. A general rule for machine learning is try to keep model as simple as possible.\n",
    "\n",
    "Using training test split, training data is for model building and cross valiadations. We use cross validation to choose our best model and then use test data, which should be completely new to the model, to test the model's ability for predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.What is k-fold cross validation and what do we use it for?\n",
    "\n",
    "ANSWER: The k-fold cross validation is the measure we apply on our training data. In a standard k-fold cross validation setting, the training data is partitioned into k folds. The model is built by using k-1 sets.The the model built is tested on the last set. We repeat this operation k times until every single set is used as the test set. The final score of the cross validation is the average of all tests. We use it to prevent overfitting (explained in the previous question) and seleciton bias (lack of randomlization in data selection). We use it as our model selection criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.How is k-fold cross validation different from stratified k-fold cross validation?\n",
    "\n",
    "ANSWER: When we have different classes in our data. We might want to use stratified k-fold to ensure each set (both train and test set) has relatively the same percentage of each class as the population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. \n",
    "\n",
    "ANSWER: The first model I use is KNN classification.\n",
    "The choice of K, I used GridSearchCV select the k of the best model based on cross validation score. The input of k is a list of odd numbers to avoid a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_freq_free:  char_freq_!:  char_freq_$:\n",
      "0             0.32         0.778         0.000\n",
      "1             0.14         0.372         0.180\n",
      "2             0.06         0.276         0.184\n",
      "3             0.31         0.137         0.000\n",
      "4             0.31         0.135         0.000\n",
      "[[ 0.08617144  0.62400658 -0.30835494]\n",
      " [-0.1318249   0.12620315  0.42378306]\n",
      " [-0.22871216  0.00849594  0.44005279]\n",
      " ...\n",
      " [-0.3013776  -0.32991229 -0.30835494]\n",
      " [-0.3013776  -0.32991229 -0.30835494]\n",
      " [-0.3013776  -0.17664769 -0.30835494]]\n"
     ]
    }
   ],
   "source": [
    "y = df['spam']  \n",
    "X_raw = pd.DataFrame(df, columns = ['word_freq_free:', 'char_freq_!:', 'char_freq_$:'])\n",
    "from sklearn import preprocessing \n",
    "X = preprocessing.scale(X_raw)\n",
    "\n",
    "print(X_raw.head())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 16)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.862\n",
      "best parameters: {'n_neighbors': 11}\n",
      "test-set score: 0.870\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid_KNN = {'n_neighbors': np.arange(1, 57, 2)}\n",
    "\n",
    "grid_KNN = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid_KNN) \n",
    "\n",
    "grid_KNN.fit(X_train, y_train) \n",
    "\n",
    "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_KNN.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_KNN.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_KNN.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?\n",
    "\n",
    "ANSWER: The second model I choose is penalized logistic regression. The input of GridSearch CV is a list all integers from 1 to 100. The best model is the one with c equal to 4.0. \n",
    "Its performance is not as good as the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.830\n",
      "best parameters: {'C': 4.0}\n",
      "test-set score: 0.840\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid_lg = {'C': np.linspace(1, 100, 100)} \n",
    "grid_lg = GridSearchCV(LogisticRegression(max_iter=100000), param_grid=param_grid_lg)\n",
    "grid_lg.fit(X_train, y_train) \n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_lg.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_lg.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_lg.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?\n",
    "\n",
    "ANSWER: The third model I use is Support vector machine. C is the squared l2 penality. The smaller the c is, the larger the penalty applies. Smaller C can soften the margin, which means to allow some points go inside the margin for better fitting. Gamma is to define the influence range of a single traing example. For smaller gammas two points can be considered similar even if are far from each other and for larger gammas we require a more strict standard for similarity. The model predicts test data worse than KNN classification model, but better than logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.848\n",
      "best parameters: {'C': 50, 'gamma': 0.001}\n",
      "test-set score: 0.854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_grid_svm = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid_svm = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid_svm)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_svm.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_svm.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_svm.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?\n",
    "\n",
    "ANSWER: The fourth model I choose is decision tree classification. I tune the parameter max_depth from 1 to 20, which controls how many times the internal nodes split and can influence predictive performance. I also tune the criterion parameter, which controls the classification criteria, because the quality of the split can also influence the model performance.\n",
    "\n",
    "Now the decision tree model is the best one out of the fourth models in terms of the test-set score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.860\n",
      "best parameters: {'criterion': 'entropy', 'max_depth': 5.0}\n",
      "test-set score: 0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid_dt = {'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.linspace(1, 20, 20)}\n",
    "\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid=param_grid_dt)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_dt.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_dt.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_dt.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_freq_free:  char_freq_!:  char_freq_$:  word_freq_address:  \\\n",
      "0             0.32         0.778         0.000                0.64   \n",
      "1             0.14         0.372         0.180                0.28   \n",
      "2             0.06         0.276         0.184                0.00   \n",
      "3             0.31         0.137         0.000                0.00   \n",
      "4             0.31         0.135         0.000                0.00   \n",
      "\n",
      "   word_freq_order:  word_freq_original:  \n",
      "0              0.00                 0.00  \n",
      "1              0.00                 0.00  \n",
      "2              0.64                 0.12  \n",
      "3              0.31                 0.00  \n",
      "4              0.31                 0.00  \n",
      "[[ 0.08617144  0.62400658 -0.30835494  0.3308849  -0.32330236 -0.20599311]\n",
      " [-0.1318249   0.12620315  0.42378306  0.05190919 -0.32330236 -0.20599311]\n",
      " [-0.22871216  0.00849594  0.44005279 -0.16507191  1.9740168   0.33022998]\n",
      " ...\n",
      " [-0.3013776  -0.32991229 -0.30835494 -0.16507191 -0.32330236 -0.20599311]\n",
      " [-0.3013776  -0.32991229 -0.30835494 -0.16507191 -0.32330236 -0.20599311]\n",
      " [-0.3013776  -0.17664769 -0.30835494 -0.16507191 -0.32330236 -0.20599311]]\n"
     ]
    }
   ],
   "source": [
    "X_raw_2 = pd.DataFrame(df, columns = ['word_freq_free:', 'char_freq_!:', 'char_freq_$:', 'word_freq_address:', 'word_freq_order:', 'word_freq_original:' ])\n",
    "from sklearn import preprocessing \n",
    "X_new = preprocessing.scale(X_raw_2)\n",
    "\n",
    "print(X_raw_2.head())\n",
    "print(X_new)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_new, y, random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.862\n",
      "best parameters: {'criterion': 'entropy', 'max_depth': 7.0}\n",
      "test-set score: 0.869\n"
     ]
    }
   ],
   "source": [
    "grid_dt_2 = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid=param_grid_dt)\n",
    "grid_dt_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_dt_2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_dt_2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_dt_2.score(X_test_2, y_test_2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The performance of this model is relatively the same as previous best decision tree mode with a little bit lower test-set score but a little bit higher CV score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  \n",
    "\n",
    "ANSWER: The final model I choose is the KNN classifier, because it has the highest score on cross validation and a good preditvie performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.871\n",
      "best parameters: {'n_neighbors': 11}\n",
      "test-set score: 0.865\n"
     ]
    }
   ],
   "source": [
    "grid_KNN_2 = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid_KNN) \n",
    "\n",
    "grid_KNN_2.fit(X_train_2, y_train_2) \n",
    "\n",
    "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_KNN_2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_KNN_2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_KNN_2.score(X_test_2, y_test_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.834\n",
      "best parameters: {'C': 2.0}\n",
      "test-set score: 0.840\n"
     ]
    }
   ],
   "source": [
    "grid_lg_2 = GridSearchCV(LogisticRegression(max_iter=100000), param_grid=param_grid_lg)\n",
    "grid_lg_2.fit(X_train_2, y_train_2) \n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_lg_2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_lg_2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_lg_2.score(X_test_2, y_test_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.858\n",
      "best parameters: {'C': 50, 'gamma': 0.001}\n",
      "test-set score: 0.862\n"
     ]
    }
   ],
   "source": [
    "grid_svm_2 = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid_svm)\n",
    "grid_svm_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_svm_2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_svm_2.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_svm_2.score(X_test_2, y_test_2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model.\n",
    "\n",
    "ANSWER: The inclusion of the appearance of \"est\",\"more\",\"most\" or \"er\" date as a variable may increse my final model's predictive power. I think spam may make comparison with the products/service of their competitors, which will lead to the appearance of \"er\" and \"more\". And they can also make exaggerated statements that invloves the use of \"est\" and \"most\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. \n",
    "\n",
    "ANSWER: Linear regression model;\n",
    "K nearest neighbor regression model;\n",
    "Ridge regression model;\n",
    "Lasso regression model;\n",
    "Decisison Tree Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
